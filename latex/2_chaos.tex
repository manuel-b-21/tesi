\chapter{Chaos in theory and experiments}
\label{chap: chaos}

\section{Dynamical systems}
\label{sec: dynamical systems}

The concept of dynamical systems is quite general, since anything that moves can be considered as a
dynamical system \cite{ref:fractal_dim}. If these changes are driven
by specific rules, we say that the system is deterministic; otherwise, if the rules
are random, it is stochastic. The main feature of chaotic systems is the fact that they are unpredictable
despite being deterministic; in order to better explain the meaning of this statement, it is essential
to involve some mathemathical definitions.

The instantaneous state of a dynamical system is described by a vector $\mathbf{s}$ within a
state space $\mathcal{S}$ (typically, $\mathcal{S}\subseteq \mathbb{R}^M$).
The state vector evolves in time according to an evolution operator $\mathcal{E}_t$ such that:
\begin{equation}
\label{eq: state space evolution}
    \mathbf{s}(t+t_0)=\mathcal{E}_t[\mathbf{s}(t_0)].
\end{equation}

In theoretical systems the state space $\mathcal{S}$ is well-defined and the evolution operator
$\mathcal{E}_t$ is given; in most cases, $\mathcal{E}_t$ is defined by a set of differential equations
which can be solved, analytically or numerically, to find the system's evolution $\mathbf{s}(t)$
out of the initial conditions $\mathbf{s}(0)$.

A dynamical system is defined to be linear if the superposition principle holds, i.e.:
\begin{equation}
\label{eq: linear systems}
    \mathcal{E}_t[c_1\mathbf{s}_1+c_2\mathbf{s_2}]=c_1\mathcal{E}_t[\mathbf{s}_1]+
    c_2\mathcal{E}_t[\mathbf{s}_2].
\end{equation}
If the last equality is not satisfied the system is said to be nonlinear.
Nonlinearity is a necessary (but not sufficient) condition for the system to be chaotic.

In experimentally observed systems the state space is not always fully accessible; moreover,
the evolution operator $\mathcal{E}_t$ is rarely known.
Observing a system consists in recording some signal $y(t)$ out of it through some measurement
function $\mathcal{M}$ acting on the system's state, i.e. $y(t)=\mathcal{M}[\mathbf{s}(t)]$.
This continuous signal is always sampled and digitized, producing a finite time series (or sequence)
$\{y_n\}$ with $n=1,...,\ell$.


\section{Formal definition of chaos}
\label{sec: chaos in theory}

An universally accepted mathematical definition of chaos does not exists but
a commonly used definition is the following, originally formulated by Robert L.
Devaney \cite{ref:chaos_definition}.

Three conditions are necessary and
sufficient to define a system as chaotic: (i) sensitivity to initial conditions;
(ii) topological transitivity;
(iii) density of periodic orbits.

Going into detail:
\begin{itemize}
    \item[(i)] Sensitivity to initial conditions is a property that characterizes chaotic
    systems and makes their evolution hard to predict.

    Given two initial conditions $\mathbf{s}_1(0)$, $\mathbf{s}_2(0)$ that are arbitrarily close within the state
    space ($||\mathbf{s}_1(0)-\mathbf{s}_2(0)||<\varepsilon$),
    the system having sensitive dependence on initial conditions means that
    the two trajectories evolving out of these initial conditions diverge exponentially in time, i.e.
    for large $t$:
    \begin{equation}
    \label{eq: trajectory divergence}
        ||\mathbf{s}_1(t)-\mathbf{s}_2(t)||\propto e^{\lambda t},
    \end{equation}
    where $\lambda$ is called maximum Lyapunov exponent (MLE).
    In order for the system to be chaotic, $\lambda$ has to be positive.

    It is also important that the orbits $\mathbf{s}_1(t)$, $\mathbf{s}_2(t)$ remain bounded at large $t$,
    otherwise, if orbits went to infinity, it would be simple for their distance to diverge exponentially.
    
    The most important consequence of this property is that, as far as we are
    able to precisely measure the initial state of a system, there will always be
    a small error (given for example by measuring instruments) which can grow
    rapidly over time. Therefore, even if we know exactly the deterministic laws
    governing time evolution, our predictions on the behaviour of the system
    after a certain time will no longer be reliable. Furthermore, if the precision
    with which we measure the state of the system in the initial instant is
    improved by a factor of $10$, we only gain a $\log(10)$ factor for the maximum
    time for which the predictions are accurate.

    \item[(ii)] Topological transitivity is the property according to which a chaotic trajectory eventually connects
    any region of the state space with any other. In other words, the state space of a chaotic system
    cannot be decomposed into disjoint subsets.

    \item[(iii)] Density of periodic orbits means that for any given point in the state space
    there is a periodic orbit that passes arbitrarily close to it, i.e. periodic orbits make up a dense set.

\end{itemize}


\section{The issue of detecting chaos}
\label{sec: chaos in experiments}

When dealing with experimentally observed systems, the precise laws that describe the dynamics are unknown.
What is known is the time series, which can be used to assess condition (i) of Devaney's definition
of chaos. Instead, conditions (ii) and (iii) are difficult to identify with the time series only.
However, there are observable consequences. In particular, the time
evolution of a chaotic system in the state space always converges to an object called strange
attractor, characterized by a fractal structure \cite{ref:fractal_dim,ref:fractal_book}. This means that strange
attractors exhibit self-similarity (or self-affinity\footnote{In general, it is more correct to speak
of self-affinity, since in the case of self-similarity the
object is scaled by the same amount in all space directions, but in self-affinity scaling is not
necessary identical in all directions \cite{ref:mandelbrot_fractal}.}) and have a non-integer dimensionality.

To understand what it means to have non-integer dimensionality, suppose
to consider a fractal object with dimension $1 < D < 2$.
We know in general that if we have an object of dimension $D$ (assuming that
it has a mass density), taking an arbitrary point in it and considering an open
ball centered in that point, we can measure the mass contained in the ball as
a function of the radius. For small distances $m(r)\propto r^D$. For the same density
and radius, the fractal ``weighs" more than a line but less then a surface, as if
a dense set (which is the set of the periodic points) had been removed from the
surface.

Self-similarity, instead, is the exhibition of similar patterns at increasingly smaller
scales; in other words, a fractal does not appear simplified when we see it zoomed. Despite strange attractors
also existing in non-chaotic systems \cite{ref:strange_attractors_non_chaotic}, the estimate of the system's non-integer dimension
is often used as a tool to identify chaos.

Another important property allows
us to identify chaos: the trajectory winds around forever never repeating on a
strange attractor and the time series arising by chaotic systems are aperiodic
and characterized by broad, noise-like Fourier spectra \cite{ref:abarbanel_fourier_spectra}.
This also means that linear techniques
such as fast Fourier transform (FFT) applied to sequences cannot distinguish between a chaotic system
and a stochastic one, e.g. a Gaussian white noise source (GWN).

One more necessary requirement to sustain a chaotic flow is that
the number of independent dimensions has to be at least three, assuming no discontinuities \cite{ref:chaos_two_dim_theorem};
this is due to the fact that
with two or less independent variables the trajectory will eventually intersect itself, which
cannot happen due to the aperiodic nature of chaotic dynamics.


\section{The embedding procedure}
\label{sec: embedding}

In experimentally observed systems it is only possible to utilize the finite time series $\{y_n\}$,
with the aim to at least obtain some invariant quantities, e.g. the maximum Lyapunov exponent or
the ``effective" dimension of the system. Under certain conditions, a procedure exists with which
it is possible to reconstruct the entire state space using only one variable, which is a
function of the state space vector $\mathbf{s}(t)$. This procedure is called ``time delay embedding''
\cite{ref:packard1980geometry}.

Embedding consists in building a sequence of $m$-dimensional vectors $\mathbf{Y}_n$ by picking
$m$ time-delayed samples of the sequence $y_n$, i.e.:
\begin{equation}
    \label{eq: embedding}
    \mathbf{Y}_n=\left(y_n,y_{n+L},y_{n+2L},...,y_{n+(m-1)L}\right),
\end{equation}
where the parameter $L$ is an integer number and is called ``lag''. It is perfectly equivalent to make use of
a ``causal'' version of Eq. \ref{eq: embedding} in which the chosen samples are $y_{n-kL}$ instead of
$y_{n+kL}$, with $k=0,...,m-1$ \cite{ref:bradley2015nonlinear}. 

It has been proven by Takens \cite{ref:takens2006detecting} and Ma{\~n}{\'e} \cite{ref:mane2006dimension} that
if the parameters $m$ (dimension) and $L$ (lag) are suitably chosen,
the reconstructed state space evolution is topologically identical to the actual state space dynamics.
This means that a ``goodâ€ embedding provides a smooth one-to-one map from the
original state space evolution to the reconstructed one, therefore enabling us to estimate the
properties of the original system that are invariant under this mapping.

The two conditions under which Takens' theorem holds regard the embedding parameters $(m,L)$.
The first one requires that $m>2D$, where $D$ is the dimension of the manifold corresponding to the
time evolution of the system, which is not known a priori; this condition can be relaxed to $m>D$ if
estimating correlation dimension \cite{ref:ding1993estimating}.
The second one states that $L$ is not a multiple of the period of any system's orbit \cite{ref:grassberger1991nonlinear}.

Aside from these two minimal conditions, since Takens' theorem is an existence theorem, it does not
give any hint on how to find the best embedding parameters. The reason for this is that as long as the
two conditions are met, every choice for the couple $(m,L)$ is good for reconstruct the dynamics;
however, this is valid only for noiseless, finely sampled and infinitely long sequences. In reality,
the issue of optimal embedding is a very active field in the physics of chaos, and several techniques
have been developed in order to overcome this problem \cite{ref:perinelli2018identification,ref:perinelli2020chasing,ref:casdagli1991state}.

%m PROBLEMS
Considering the dimension $m$, choosing $m\gg1$ ensures, in principle, that $m$ is
greater than $2D$. However, if $m$ is too large the directions orthogonal to the deterministic noiseless
trajectory will be dominated by noise and will not provide extra information; in other words, supposing
that $m_0$ is the minimum dimension for which the system is correctly embedded, the remaining $m-m_0$
dimensions would be populated by noise, being thus redundant and resulting in an increase of the
computational cost. Moreover, the larger is the embedding dimension $m$,
the fewer independent embedding vectors are available, which is an issue considering the finiteness
of the time series.

%L PROBLEMS
The choice on the lag $L$ is also not trivial. If $L$ is too short, the elements of the embedding vectors
$\mathbf{Y}_n$ will be strongly correlated, resulting in all the points being clustered on the
diagonal of the reconstructed space; in presence of noise, therefore, the trajectory would be
indistinguishable from the diagonal itself. On the other hand, a too large $L$ presents the opposite
problem, i.e. the elements of $\mathbf{Y}_n$ to be completely uncorrelated with each other;
this implies that the time evolution is ``blurred'', in the sense that the trajectory is folded over on itself
and the system evolution is lost.


\section{Correlation dimension}
\label{sec: correlation dimension}

Before addressing the issue of optimal embedding, it is important to formally define the invariant
quantities that will be used to decide if an experimentally observed system is chaotic or not,
the first one being the correlation dimension \cite{ref:abarbanel_fourier_spectra}.

The simplest definition of dimension is the (integer) number of coordinates that are
needed to specify a state, e.g. the number of differential equations of a system.
This is the geometrically related to the concept of how (hyper) volumes scale as a
function of a characteristic length parameter.
One way to estimate the dimension of a set of points is the box-counting method, which consists in partitioning the space in
hypercubes of size $l$ and by counting, as a function of $l$, the fraction $\eta(l)$ of
these hypercubes containing at least one point of the set. The dimension can then be calculated
using the fact that $\eta(l\rightarrow0)\propto l^D$. Since this method is
very sensitive and computationally demanding \cite{ref:greenside1982impracticality},
it is useful to search for other ways to estimate the dimension.

Given a set of points $\{\mathbf{Y}_n\}$ within an $m$-dimensional space partitioned in hypercubes
of size $l$, and given $p_i$
the fraction of points of the set that fall within the $i$-th hypercube,
the generalized dimension is defined as \cite{ref:fractal_dim}:
\begin{equation}
    \label{eq: generalized dimension}
    D_q=\lim_{l\rightarrow0}\frac{1}{q-1}\frac{\log(\sum_i p_i^q)}{\log l}.
\end{equation}

This definition provides a whole spectrum of invariant quantities for $-\infty<q<\infty$.
For example, $D_0$ is exactly the dimension calculated with the box-counting method, i.e.:
\begin{equation}
    \label{eq: D_0}
    D_0=\lim_{l\rightarrow0}\frac{\log(\sum_i1)}{\log l}
    =\lim_{l\rightarrow0}\frac{\log\eta(l)}{\log l}.
\end{equation}

The dimension for $q\rightarrow1$ can instead be calculated using L'Hospital's rule, resulting in:
\begin{equation}
    \label{eq: D_1}
    D_1=\lim_{l\rightarrow0}\frac{\sum_i p_i\log p_i}{\log l}.
\end{equation}
$D_1$ is called informational dimension \cite{ref:abarbanel_fourier_spectra} due to the numerator
being related to information entropy. In a uniform fractal, in which
$p_i=1/N$ for every $i$, $D_1=D_0$; instead, if the $p_i$ are different $D_1<D_0$.
This is true in general for every $q$, namely $D_i\leq D_j$ if $i>j$, when the equality only
holds in a uniform fractal.

Finally, one of the most utilized dimensions in chaos theory is the correlation dimension, defined as:
\begin{equation}
    \label{eq: D_2}
    D_2=\lim_{l\rightarrow0}\frac{\log(\sum_i p_i^2)}{\log l}.
\end{equation}
The numerator constitutes a two-point correlation function, measuring the probability of finding a
pair of random points within a given partition element, just as the numerator in the definition of
$D_1$ measures the probability of finding one point in a given element.

The reason why correlation dimension is so utilized lies in the fact that there is a very
efficient way to estimate it, observed by Grassberger and Procaccia \cite{ref:grassberger1983measuring}.
Due to the exponential divergence of the trajectories, most pairs
$(\mathbf{Y}_i,\mathbf{Y}_j)$ with $i\neq j$ will be dynamically uncorrelated
pairs of essentially random points. The points lie however on the attractor, therefore they will be
spatially correlated. This spatial correlation can be measured with the correlation integral
$C(r)$, defined as:
\begin{equation}
    \label{eq: correlation integral}
    C(r) = \lim_{\ell\rightarrow\infty} \frac{1}{\ell^2}\times\left\{
        \text{number of pairs $(i,j)$ whose distance is less than $r$}
    \right\},
\end{equation}
where $\ell$ is the total number of points in the reconstructed space.
Grassberger and Procaccia proved that for small distances the correlation integral grows as
$C(r)\propto r^{D_2}$. It is thus possible to estimate the correlation dimension
by extracting random pairs of vectors in the embedded sequence, evaluating then a ``sample''
version of Eq. \ref{eq: correlation integral}, namely:
\begin{equation}
    \label{eq: sample correlation integral}
    C_{m,L}(r)=\frac{1}{N_{\text{pairs}}}\sum_{i,j}\theta\left(
        r-||\mathbf{Y}_i-\mathbf{Y}_j||
    \right),
\end{equation}
where $\theta(x)$ is the Heaviside function and the subscript $_{m,L}$ indicates the dependence of this
sample correlation integral on the embedding parameters. 

A source of error regarding the estimation of $C(r)$ are temporal correlations within the input sequence,
which can lead to underestimates of the dimension $D_2$ \cite{ref:theiler1986spurious}
or spurious contributions even in the case of stochastic sequences \cite{ref:osborne1989finite}.
This issue can be avoided by selecting pairs of points that are distant in time more than some delay
$c_0$, i.e. constraining $|i-j|\geq c_0$. Typically, $c_0$ is chosen to be the first zero of the
autocorrelation function \cite{ref:theiler1986spurious} or its first minimum \cite{ref:albano1995kolmogorov}.



\section{Maximum Lyapunov exponent}
\label{sec: mle}

The second fundamental invariant quantity that characterizes a chaotic system is the maximum Lyapunov
exponent (MLE) \cite{ref:abarbanel_fourier_spectra}.
There are as many Lyapunov exponents as the number of state space dimensions: each exponent $\lambda_i$
corresponds to one of the independent directions along which fiducial volumes within the state space
contract ($\lambda_i<0$) or expand ($\lambda_i>0$) in an infinitesimal time interval.
The MLE $\lambda_1$ is the most relevant of the spectrum, since its sign establishes whether the system
stabilizes on a fixed point ($\lambda_1<0$), stablizes on a limit cycle ($\lambda_1=0$) or is unstable
($\lambda_1>0$). A positive MLE is the main hallmark of chaos.

It is possible to estimate the MLE through the embedded sequence by making use of the so-called
divergence rate method \cite{ref:gao1993local}. This method
is based on the evaluation of the time-dependent divergence exponent $\Lambda(k)$,
which quantifies the average separation of nearby trajectories.
The estimation of this exponent is done by selecting random pairs $(i,j)$ of neighboring vectors
in the embedded sequence $\{\mathbf{Y}_n\}$, i.e. such that $||\mathbf{Y}_i-\mathbf{Y}_j||<r$,
where $r$ is some ``shell radius''; the divergence exponent is then calculated as:
\begin{equation}
    \label{eq: divergence exponent}
    \Lambda_{m,L}(k)=\frac{1}{N_{\text{pairs}}}\sum_{i,j}\log\left(
        \frac{||\mathbf{Y}_{i+k}-\mathbf{Y}_{j+k}||}{||\mathbf{Y}_i-\mathbf{Y}_j||}
    \right),
\end{equation}
where the subscript $_{m,L}$ indicates once again the dependence on the embedding parameters.

The time-dependent divergence exponent
measures the progressive separation, as a function of the time delay $k$, of the trajectories corresponding
to initially close points $\mathbf{Y}_i,\mathbf{Y}_j$.
The initial proximity of the vector pairs is tuned by changing the shell
radius $r$, which is typically estimated as the distance corresponding to the a given percentile $p$ of the
sample distribution of all Euclidean distances 
\cite{ref:franchi2014statistical,ref:ricci2020asymptotic}.

Like in Section \ref{sec: correlation dimension}, it is important that the time separation between
the pairs $(i,j)$ is greater than the autocorrelation time, i.e. $|i-j|\geq c_0$ 
\cite{ref:theiler1986spurious,ref:albano1995kolmogorov}.

If the underlying system is chaotic, $\Lambda(k)$ is expected to grow linearly with $k$,
as a result of the exponentially increasing numerator within the logarithm of Eq. \ref{eq: divergence exponent}.
Suitably fitting this linear growth provides the value of the MLE.
The growth of $\Lambda(k)$ cannot continue for arbitrarily large values of $k$:
eventually, the separation becomes comparable to the size of the attractor
and the divergence exponent saturates.


\section{A method for detecting chaos}
\label{sec: method for chaos}

The most utilized tools for detecting chaos, namely the embedding procedure, the correlation dimension
and the maximum Lyapunov exponent, have now been introduced.
It is now possible to establish an efficient procedure which not only searches for the best
embedding parameters $(m,L)$, but also provides estimates for the invariant quantities that
characterize chaos \cite{ref:perinelli2020chasing}.

\subsection{The embedding lattice}
\label{subsec: embedding lattice}

The scalar sequence $\{y_n\}$ is obtained by sampling the signal $y(t)$ with a sampling time $T$;
the time series is then supposed to be standardized as
$y \rightarrow (y - \bar{y})/\bar{\sigma}$, where $\bar{y}$ and $\bar{\sigma}$ are the sequence's
sample mean and sample standard deviation, respectively.

Since a single optimal $(m,L)$ does
not necessarily exist, the embedding of the sequence is carried out for several values of the
embedding parameters, considering that a set of
$(m, L)$ can provide reconstructions of the underlying dynamics that are equivalently good.
More specifically, the procedure is carried out for each element of an embedding lattice, i.e.
$\{(m,L)|m\in[2,m_{\max}],L\in[1,L_{\max}]\}$.

Suitable embedding choices are expected to comply with two requirements concerning the
corresponding embedding window $w=(m-1)LT$, which corresponds to the
time span covered by each embedding vector.
On the one hand, the embedding window has to be larger than the redundance time $\tau_R$, so that
the points in the reconstructed space are not too close to the diagonal, as discussed in Section
\ref{sec: embedding}. On the other hand, the embedding window has to be
smaller than the irrelevance time $\tau_I$, after which the points are causally disconnected
and the noise overcomes the dynamics.

The above requirements can then be expressed as $\tau_R/T \lesssim w \lesssim \tau_I/T$.
In the embedding lattice, each window $w$ identifies a hyperbola.
Therefore, the irrelevance and redundance times correspond to two hyperbolae
within the embedding lattice, and the region bounded by them is where suitable
embedding choices can be expected.


\subsection{The divergence exponent}
\label{subsec: divergence exponent}

The next step of the procedure consists in calculating the maximum Lyapunov exponent and
the correlation dimension. An efficient manner to do this is making use of the divergence exponent
$\Lambda(k)$. As was already discussed in Section \ref{sec: mle}, if $\Lambda(k)$ exhibits a
linear behavior it is possible to estimate the MLE as the slope of the curve.

For sufficiently large values of $k$, the divergence exponent $\Lambda(k)$
reaches a saturation value $\Lambda_{\text{pl}}$, referred to as a ``plateau".
In the case of a chaotic source of finite correlation dimension $\nu$, the plateau
turns out to linearly depend on the logarithm of the percentile $p$ \cite{ref:perinelli2020chasing}:
\begin{equation}
    \label{eq: Lambda plateau}
    \Lambda_{\text{pl}}=\Lambda'-\frac{1}{\nu}\log p,
\end{equation}
where $\Lambda'$ is a constant that depends on the embedding parameters.





\begin{comment}



\end{comment}